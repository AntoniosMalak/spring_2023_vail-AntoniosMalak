# Optimization

## Topics covered in today's module
* Optimization
* Gradident Descent
* Optimizers(SGD, ADAM, etc.)

## Main takeaways from doing today's assignment
* Optimization
  - What is Optimization and how does it connect to loss and activation funcations.
* Gradident Descent
  - How does it work and why we thinking in GD.
* Optimizers(SGD, ADAM, etc.)
  - Use differnet optimizers like `RMSProp`, `SGD`, `Adam` and `AdaGrad`.
  - How does each of the optimizers above effect the model?

## Challenging, interesting, or exciting aspects of today's assignment
After the last day use loss functions, this is an amazing day to use optimizers and how they connect with losses and activation functions and use different optimizers.
## Additional resources used 
- [Optimizers](https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0)
- [Gradident Descent](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)

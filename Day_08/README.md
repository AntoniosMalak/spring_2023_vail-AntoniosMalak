# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
* Introduction to Sigmoid, Tanh, ReLU
  - What is the difference between these activation functions?
  - What will happen when we use any activation functions.
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
  - See what is problem of exploding and vanishing gradients.
* Dying ReLU problem
  - What ReLU can do?
* Advanced activation functions: Swish, GeLU, SeLU
  - Swish, GeLU, SeLU and what difference between them and regular activation functions.

## Challenging, interesting, or exciting aspects of today's assignment
It's an interesting topic that makes us work with activation functions and see their effect on our model.

## Additional resources used 
- [Visualizing intermediate activation in CNN](https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0)
- [Activation Functions in Neural Networks [12 Types & Use Cases]](https://www.v7labs.com/blog/neural-networks-activation-functions)
- [Activation Functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)
- [How to Choose an Activation Function for Deep Learning](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)

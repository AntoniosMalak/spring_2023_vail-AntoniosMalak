{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MySureStart/spring_2023_vail-AntoniosMalak/blob/main/Day_05/Introduction_to_Regression_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 5 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "outputId": "78068483-fb84-4a7f-c3dd-9b616521f58c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "history = model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 3s 32ms/step - loss: 597.7871 - mse: 597.7871 - val_loss: 500.4353 - val_mse: 500.4353\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 585.8685 - mse: 585.8685 - val_loss: 490.8345 - val_mse: 490.8345\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 573.8472 - mse: 573.8472 - val_loss: 479.1485 - val_mse: 479.1485\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 558.0491 - mse: 558.0491 - val_loss: 462.4086 - val_mse: 462.4086\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 535.1863 - mse: 535.1863 - val_loss: 438.8786 - val_mse: 438.8786\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 503.0319 - mse: 503.0319 - val_loss: 406.7541 - val_mse: 406.7541\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 459.6786 - mse: 459.6786 - val_loss: 364.0350 - val_mse: 364.0350\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 401.9572 - mse: 401.9572 - val_loss: 308.7784 - val_mse: 308.7784\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 331.8801 - mse: 331.8801 - val_loss: 238.9847 - val_mse: 238.9847\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 247.3089 - mse: 247.3089 - val_loss: 162.4128 - val_mse: 162.4128\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 159.3795 - mse: 159.3795 - val_loss: 88.9914 - val_mse: 88.9914\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 89.3945 - mse: 89.3945 - val_loss: 42.7946 - val_mse: 42.7946\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 51.7158 - mse: 51.7158 - val_loss: 30.8233 - val_mse: 30.8233\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 42.2232 - mse: 42.2232 - val_loss: 28.6350 - val_mse: 28.6350\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 39.5343 - mse: 39.5343 - val_loss: 26.7048 - val_mse: 26.7048\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 37.0487 - mse: 37.0487 - val_loss: 24.8334 - val_mse: 24.8334\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 35.1352 - mse: 35.1352 - val_loss: 23.9040 - val_mse: 23.9040\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 33.4771 - mse: 33.4771 - val_loss: 23.2542 - val_mse: 23.2542\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 32.1844 - mse: 32.1844 - val_loss: 22.0370 - val_mse: 22.0370\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 30.8812 - mse: 30.8812 - val_loss: 21.3123 - val_mse: 21.3123\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29.8037 - mse: 29.8037 - val_loss: 20.9740 - val_mse: 20.9740\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28.9630 - mse: 28.9630 - val_loss: 20.0771 - val_mse: 20.0771\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 27.9993 - mse: 27.9993 - val_loss: 19.7285 - val_mse: 19.7285\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 27.1704 - mse: 27.1704 - val_loss: 19.6162 - val_mse: 19.6162\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 26.4725 - mse: 26.4725 - val_loss: 18.7429 - val_mse: 18.7429\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 25.6823 - mse: 25.6823 - val_loss: 18.3284 - val_mse: 18.3284\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 25.1348 - mse: 25.1348 - val_loss: 17.8737 - val_mse: 17.8737\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 24.4778 - mse: 24.4778 - val_loss: 17.8404 - val_mse: 17.8404\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 23.9898 - mse: 23.9898 - val_loss: 17.3073 - val_mse: 17.3073\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 23.5521 - mse: 23.5521 - val_loss: 16.6285 - val_mse: 16.6285\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 22.9333 - mse: 22.9333 - val_loss: 16.3961 - val_mse: 16.3961\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 22.6193 - mse: 22.6193 - val_loss: 16.6151 - val_mse: 16.6151\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 22.0566 - mse: 22.0566 - val_loss: 16.1439 - val_mse: 16.1439\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 21.8450 - mse: 21.8450 - val_loss: 15.5745 - val_mse: 15.5745\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 21.3404 - mse: 21.3404 - val_loss: 15.9963 - val_mse: 15.9963\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 20.9747 - mse: 20.9747 - val_loss: 15.4142 - val_mse: 15.4142\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 20.5161 - mse: 20.5161 - val_loss: 14.8693 - val_mse: 14.8693\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 20.3251 - mse: 20.3251 - val_loss: 14.8826 - val_mse: 14.8826\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 19.9030 - mse: 19.9030 - val_loss: 14.6377 - val_mse: 14.6377\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 19.5574 - mse: 19.5574 - val_loss: 14.3770 - val_mse: 14.3770\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 19.2699 - mse: 19.2699 - val_loss: 14.3943 - val_mse: 14.3943\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 18.9502 - mse: 18.9502 - val_loss: 14.4382 - val_mse: 14.4382\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 18.7549 - mse: 18.7549 - val_loss: 14.1696 - val_mse: 14.1696\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 18.4673 - mse: 18.4673 - val_loss: 13.9758 - val_mse: 13.9758\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 18.3367 - mse: 18.3367 - val_loss: 14.0418 - val_mse: 14.0418\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 17.9340 - mse: 17.9340 - val_loss: 13.6208 - val_mse: 13.6208\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 17.7636 - mse: 17.7636 - val_loss: 13.5273 - val_mse: 13.5273\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 17.5830 - mse: 17.5830 - val_loss: 13.6762 - val_mse: 13.6762\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 17.2083 - mse: 17.2083 - val_loss: 13.1142 - val_mse: 13.1142\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 16.9801 - mse: 16.9801 - val_loss: 13.0480 - val_mse: 13.0480\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 16.7109 - mse: 16.7109 - val_loss: 12.7624 - val_mse: 12.7624\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 16.4941 - mse: 16.4941 - val_loss: 12.6440 - val_mse: 12.6440\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 16.5820 - mse: 16.5820 - val_loss: 12.8542 - val_mse: 12.8542\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 16.0997 - mse: 16.0997 - val_loss: 12.4992 - val_mse: 12.4992\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 15.8839 - mse: 15.8839 - val_loss: 12.4681 - val_mse: 12.4681\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 15.6743 - mse: 15.6743 - val_loss: 12.3115 - val_mse: 12.3115\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 15.4735 - mse: 15.4735 - val_loss: 12.2705 - val_mse: 12.2705\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 15.3206 - mse: 15.3206 - val_loss: 12.0870 - val_mse: 12.0870\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 15.1478 - mse: 15.1478 - val_loss: 11.9305 - val_mse: 11.9305\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 14.8991 - mse: 14.8991 - val_loss: 12.0003 - val_mse: 12.0003\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 14.7765 - mse: 14.7765 - val_loss: 11.6814 - val_mse: 11.6814\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 14.4913 - mse: 14.4913 - val_loss: 11.5084 - val_mse: 11.5084\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 14.3343 - mse: 14.3343 - val_loss: 11.4153 - val_mse: 11.4153\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 14.1426 - mse: 14.1426 - val_loss: 11.2405 - val_mse: 11.2405\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 13.9551 - mse: 13.9551 - val_loss: 11.2082 - val_mse: 11.2082\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 13.7790 - mse: 13.7790 - val_loss: 11.1259 - val_mse: 11.1259\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 13.6142 - mse: 13.6142 - val_loss: 11.1133 - val_mse: 11.1133\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 13.3856 - mse: 13.3856 - val_loss: 10.9828 - val_mse: 10.9828\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 13.1811 - mse: 13.1811 - val_loss: 10.8516 - val_mse: 10.8516\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 13.0756 - mse: 13.0756 - val_loss: 10.8524 - val_mse: 10.8524\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 12.8509 - mse: 12.8509 - val_loss: 10.3848 - val_mse: 10.3848\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 12.7051 - mse: 12.7051 - val_loss: 10.3598 - val_mse: 10.3598\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 12.5445 - mse: 12.5445 - val_loss: 10.2957 - val_mse: 10.2957\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 12.4350 - mse: 12.4350 - val_loss: 10.1552 - val_mse: 10.1552\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 12.3279 - mse: 12.3279 - val_loss: 9.9569 - val_mse: 9.9569\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 12.0528 - mse: 12.0528 - val_loss: 9.9216 - val_mse: 9.9216\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 12.1599 - mse: 12.1599 - val_loss: 9.9008 - val_mse: 9.9008\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 11.8880 - mse: 11.8880 - val_loss: 9.7326 - val_mse: 9.7326\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 11.6992 - mse: 11.6992 - val_loss: 9.4931 - val_mse: 9.4931\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 11.5507 - mse: 11.5507 - val_loss: 9.4990 - val_mse: 9.4990\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 11.4253 - mse: 11.4253 - val_loss: 9.2398 - val_mse: 9.2398\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 11.3381 - mse: 11.3381 - val_loss: 9.2182 - val_mse: 9.2182\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 11.1303 - mse: 11.1303 - val_loss: 9.1503 - val_mse: 9.1503\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 11.1870 - mse: 11.1870 - val_loss: 8.9349 - val_mse: 8.9349\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 10.9417 - mse: 10.9417 - val_loss: 8.9253 - val_mse: 8.9253\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 10.9049 - mse: 10.9049 - val_loss: 8.9498 - val_mse: 8.9498\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 10.7482 - mse: 10.7482 - val_loss: 8.7642 - val_mse: 8.7642\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 10.6389 - mse: 10.6389 - val_loss: 8.6609 - val_mse: 8.6609\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 10.5449 - mse: 10.5449 - val_loss: 8.7827 - val_mse: 8.7827\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 10.3925 - mse: 10.3925 - val_loss: 8.5505 - val_mse: 8.5505\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 10.3343 - mse: 10.3343 - val_loss: 8.5458 - val_mse: 8.5458\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 10.1960 - mse: 10.1960 - val_loss: 8.4548 - val_mse: 8.4548\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 10.1571 - mse: 10.1571 - val_loss: 8.2731 - val_mse: 8.2731\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 10.0468 - mse: 10.0468 - val_loss: 8.0203 - val_mse: 8.0203\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 10.0450 - mse: 10.0450 - val_loss: 8.2125 - val_mse: 8.2125\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 9.8902 - mse: 9.8902 - val_loss: 8.1429 - val_mse: 8.1429\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 9.7960 - mse: 9.7960 - val_loss: 8.0136 - val_mse: 8.0136\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 9.7300 - mse: 9.7300 - val_loss: 7.9334 - val_mse: 7.9334\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 9.6797 - mse: 9.6797 - val_loss: 7.7921 - val_mse: 7.7921\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 9.6398 - mse: 9.6398 - val_loss: 7.8340 - val_mse: 7.8340\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 9.5132 - mse: 9.5132 - val_loss: 7.7862 - val_mse: 7.7862\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 9.4864 - mse: 9.4864 - val_loss: 7.7950 - val_mse: 7.7950\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 9.4268 - mse: 9.4268 - val_loss: 7.6532 - val_mse: 7.6532\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 9.3673 - mse: 9.3673 - val_loss: 7.7021 - val_mse: 7.7021\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 9.3331 - mse: 9.3331 - val_loss: 7.6426 - val_mse: 7.6426\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 9.3778 - mse: 9.3778 - val_loss: 7.3178 - val_mse: 7.3178\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 9.2692 - mse: 9.2692 - val_loss: 7.4599 - val_mse: 7.4599\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 9.1024 - mse: 9.1024 - val_loss: 7.6730 - val_mse: 7.6730\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 9.0736 - mse: 9.0736 - val_loss: 7.4754 - val_mse: 7.4754\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.9871 - mse: 8.9871 - val_loss: 7.4155 - val_mse: 7.4155\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 8.9548 - mse: 8.9548 - val_loss: 7.2569 - val_mse: 7.2569\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.9691 - mse: 8.9691 - val_loss: 7.0444 - val_mse: 7.0444\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 8.9146 - mse: 8.9146 - val_loss: 7.2461 - val_mse: 7.2461\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 8.8981 - mse: 8.8981 - val_loss: 7.2449 - val_mse: 7.2449\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 8.7538 - mse: 8.7538 - val_loss: 7.1202 - val_mse: 7.1202\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 8.7416 - mse: 8.7416 - val_loss: 7.2734 - val_mse: 7.2734\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 8.6727 - mse: 8.6727 - val_loss: 6.8944 - val_mse: 6.8944\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.6300 - mse: 8.6300 - val_loss: 6.6863 - val_mse: 6.6863\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.6077 - mse: 8.6077 - val_loss: 7.1610 - val_mse: 7.1610\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.6354 - mse: 8.6354 - val_loss: 6.8850 - val_mse: 6.8850\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.5671 - mse: 8.5671 - val_loss: 6.8267 - val_mse: 6.8267\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 8.6040 - mse: 8.6040 - val_loss: 6.8495 - val_mse: 6.8495\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.3694 - mse: 8.3694 - val_loss: 6.7441 - val_mse: 6.7441\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.4316 - mse: 8.4316 - val_loss: 6.7777 - val_mse: 6.7777\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 8.5222 - mse: 8.5222 - val_loss: 6.3465 - val_mse: 6.3465\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.2774 - mse: 8.2774 - val_loss: 6.8017 - val_mse: 6.8017\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.3314 - mse: 8.3314 - val_loss: 6.6690 - val_mse: 6.6690\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 8.3374 - mse: 8.3374 - val_loss: 6.5604 - val_mse: 6.5604\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.2761 - mse: 8.2761 - val_loss: 6.7324 - val_mse: 6.7324\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.2482 - mse: 8.2482 - val_loss: 6.7566 - val_mse: 6.7566\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 8.2078 - mse: 8.2078 - val_loss: 6.6338 - val_mse: 6.6338\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 8.0885 - mse: 8.0885 - val_loss: 6.6671 - val_mse: 6.6671\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 8.0795 - mse: 8.0795 - val_loss: 6.5637 - val_mse: 6.5637\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 8.0707 - mse: 8.0707 - val_loss: 6.4251 - val_mse: 6.4251\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 8.0284 - mse: 8.0284 - val_loss: 6.2646 - val_mse: 6.2646\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 8.0018 - mse: 8.0018 - val_loss: 6.5648 - val_mse: 6.5648\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 7.9263 - mse: 7.9263 - val_loss: 6.3997 - val_mse: 6.3997\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 7.9644 - mse: 7.9644 - val_loss: 6.3107 - val_mse: 6.3107\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 7.9208 - mse: 7.9208 - val_loss: 6.6020 - val_mse: 6.6020\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 7.9108 - mse: 7.9108 - val_loss: 6.2948 - val_mse: 6.2948\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 7.9120 - mse: 7.9120 - val_loss: 6.5025 - val_mse: 6.5025\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 7.8571 - mse: 7.8571 - val_loss: 6.4743 - val_mse: 6.4743\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8118 - mse: 7.8118 - val_loss: 6.3324 - val_mse: 6.3324\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8977 - mse: 7.8977 - val_loss: 6.2572 - val_mse: 6.2572\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7865 - mse: 7.7865 - val_loss: 6.5248 - val_mse: 6.5248\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7000 - mse: 7.7000 - val_loss: 6.1780 - val_mse: 6.1780\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.7172 - mse: 7.7172 - val_loss: 6.2543 - val_mse: 6.2543\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6503 - mse: 7.6503 - val_loss: 6.1874 - val_mse: 6.1874\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.7158 - mse: 7.7158 - val_loss: 6.1683 - val_mse: 6.1683\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7116 - mse: 7.7116 - val_loss: 6.3106 - val_mse: 6.3106\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6125 - mse: 7.6125 - val_loss: 5.9602 - val_mse: 5.9602\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6017 - mse: 7.6017 - val_loss: 6.0738 - val_mse: 6.0738\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6139 - mse: 7.6139 - val_loss: 6.1007 - val_mse: 6.1007\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.6588 - mse: 7.6588 - val_loss: 6.3332 - val_mse: 6.3332\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7123 - mse: 7.7123 - val_loss: 5.9978 - val_mse: 5.9978\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.6261 - mse: 7.6261 - val_loss: 6.1774 - val_mse: 6.1774\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.4420 - mse: 7.4420 - val_loss: 5.9297 - val_mse: 5.9297\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4458 - mse: 7.4458 - val_loss: 6.1087 - val_mse: 6.1087\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.4334 - mse: 7.4334 - val_loss: 6.0041 - val_mse: 6.0041\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.4026 - mse: 7.4026 - val_loss: 6.0075 - val_mse: 6.0075\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3935 - mse: 7.3935 - val_loss: 6.0309 - val_mse: 6.0309\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3520 - mse: 7.3520 - val_loss: 5.8833 - val_mse: 5.8833\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3382 - mse: 7.3382 - val_loss: 5.9331 - val_mse: 5.9331\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.3064 - mse: 7.3064 - val_loss: 6.2790 - val_mse: 6.2790\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.4940 - mse: 7.4940 - val_loss: 5.8055 - val_mse: 5.8055\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2868 - mse: 7.2868 - val_loss: 6.0760 - val_mse: 6.0760\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.2609 - mse: 7.2609 - val_loss: 5.7804 - val_mse: 5.7804\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2312 - mse: 7.2312 - val_loss: 6.1038 - val_mse: 6.1038\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2307 - mse: 7.2307 - val_loss: 5.8615 - val_mse: 5.8615\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1782 - mse: 7.1782 - val_loss: 5.7422 - val_mse: 5.7422\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1653 - mse: 7.1653 - val_loss: 5.7874 - val_mse: 5.7874\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1829 - mse: 7.1829 - val_loss: 5.7557 - val_mse: 5.7557\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.1417 - mse: 7.1417 - val_loss: 5.8487 - val_mse: 5.8487\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.1937 - mse: 7.1937 - val_loss: 6.1225 - val_mse: 6.1225\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1072 - mse: 7.1072 - val_loss: 5.8355 - val_mse: 5.8355\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.0491 - mse: 7.0491 - val_loss: 5.7972 - val_mse: 5.7972\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0809 - mse: 7.0809 - val_loss: 5.8726 - val_mse: 5.8726\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0616 - mse: 7.0616 - val_loss: 5.9560 - val_mse: 5.9560\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.0727 - mse: 7.0727 - val_loss: 5.6145 - val_mse: 5.6145\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9947 - mse: 6.9947 - val_loss: 5.8425 - val_mse: 5.8425\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9771 - mse: 6.9771 - val_loss: 5.7170 - val_mse: 5.7170\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1676 - mse: 7.1676 - val_loss: 6.1211 - val_mse: 6.1211\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9759 - mse: 6.9759 - val_loss: 5.5690 - val_mse: 5.5690\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9368 - mse: 6.9368 - val_loss: 5.8230 - val_mse: 5.8230\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.8921 - mse: 6.8921 - val_loss: 5.8224 - val_mse: 5.8224\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9669 - mse: 6.9669 - val_loss: 5.7533 - val_mse: 5.7533\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9135 - mse: 6.9135 - val_loss: 6.0133 - val_mse: 6.0133\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.8355 - mse: 6.8355 - val_loss: 5.8432 - val_mse: 5.8432\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.8597 - mse: 6.8597 - val_loss: 5.5769 - val_mse: 5.5769\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9303 - mse: 6.9303 - val_loss: 5.8168 - val_mse: 5.8168\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9329 - mse: 6.9329 - val_loss: 5.4996 - val_mse: 5.4996\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.7595 - mse: 6.7595 - val_loss: 5.8368 - val_mse: 5.8368\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8395 - mse: 6.8395 - val_loss: 5.7721 - val_mse: 5.7721\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.7204 - mse: 6.7204 - val_loss: 5.6672 - val_mse: 5.6672\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.7211 - mse: 6.7211 - val_loss: 5.8204 - val_mse: 5.8204\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.7010 - mse: 6.7010 - val_loss: 5.7225 - val_mse: 5.7225\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.7057 - mse: 6.7057 - val_loss: 5.5032 - val_mse: 5.5032\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8418 - mse: 6.8418 - val_loss: 5.6613 - val_mse: 5.6613\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.7380 - mse: 6.7380 - val_loss: 5.6369 - val_mse: 5.6369\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.8399 - mse: 6.8399 - val_loss: 5.6876 - val_mse: 5.6876\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.6107 - mse: 6.6107 - val_loss: 5.6589 - val_mse: 5.6589\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.5875 - mse: 6.5875 - val_loss: 5.5920 - val_mse: 5.5920\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7903 - mse: 6.7903 - val_loss: 5.5907 - val_mse: 5.5907\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.6700 - mse: 6.6700 - val_loss: 5.7420 - val_mse: 5.7420\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.6681 - mse: 6.6681 - val_loss: 5.7873 - val_mse: 5.7873\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.6760 - mse: 6.6760 - val_loss: 5.7446 - val_mse: 5.7446\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.5309 - mse: 6.5309 - val_loss: 5.5325 - val_mse: 5.5325\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5542 - mse: 6.5542 - val_loss: 5.6234 - val_mse: 5.6234\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.5014 - mse: 6.5014 - val_loss: 5.7117 - val_mse: 5.7117\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.4966 - mse: 6.4966 - val_loss: 5.5871 - val_mse: 5.5871\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.7071 - mse: 6.7071 - val_loss: 5.3596 - val_mse: 5.3596\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.5617 - mse: 6.5617 - val_loss: 5.7389 - val_mse: 5.7389\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.6423 - mse: 6.6423 - val_loss: 5.8686 - val_mse: 5.8686\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5931 - mse: 6.5931 - val_loss: 5.4399 - val_mse: 5.4399\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4578 - mse: 6.4578 - val_loss: 5.9011 - val_mse: 5.9011\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3872 - mse: 6.3872 - val_loss: 5.6450 - val_mse: 5.6450\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4082 - mse: 6.4082 - val_loss: 5.4570 - val_mse: 5.4570\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3411 - mse: 6.3411 - val_loss: 5.6980 - val_mse: 5.6980\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.3531 - mse: 6.3531 - val_loss: 5.6863 - val_mse: 5.6863\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.6399 - mse: 6.6399 - val_loss: 5.7842 - val_mse: 5.7842\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.4609 - mse: 6.4609 - val_loss: 5.4189 - val_mse: 5.4189\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.4342 - mse: 6.4342 - val_loss: 5.7231 - val_mse: 5.7231\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2816 - mse: 6.2816 - val_loss: 5.6441 - val_mse: 5.6441\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.2817 - mse: 6.2817 - val_loss: 5.8443 - val_mse: 5.8443\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.1918 - mse: 6.1918 - val_loss: 5.4336 - val_mse: 5.4336\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.2952 - mse: 6.2952 - val_loss: 5.4405 - val_mse: 5.4405\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2445 - mse: 6.2445 - val_loss: 5.8238 - val_mse: 5.8238\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.2258 - mse: 6.2258 - val_loss: 5.5801 - val_mse: 5.5801\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2978 - mse: 6.2978 - val_loss: 5.6568 - val_mse: 5.6568\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1572 - mse: 6.1572 - val_loss: 5.6124 - val_mse: 5.6124\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1798 - mse: 6.1798 - val_loss: 5.3371 - val_mse: 5.3371\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.3037 - mse: 6.3037 - val_loss: 5.8185 - val_mse: 5.8185\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2328 - mse: 6.2328 - val_loss: 5.5731 - val_mse: 5.5731\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2201 - mse: 6.2201 - val_loss: 5.6775 - val_mse: 5.6775\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0126 - mse: 6.0126 - val_loss: 5.6942 - val_mse: 5.6942\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.0437 - mse: 6.0437 - val_loss: 5.6761 - val_mse: 5.6761\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0086 - mse: 6.0086 - val_loss: 5.5475 - val_mse: 5.5475\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1023 - mse: 6.1023 - val_loss: 5.4087 - val_mse: 5.4087\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9888 - mse: 5.9888 - val_loss: 5.7828 - val_mse: 5.7828\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.9886 - mse: 5.9886 - val_loss: 5.6650 - val_mse: 5.6650\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.9409 - mse: 5.9409 - val_loss: 5.6812 - val_mse: 5.6812\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9469 - mse: 5.9469 - val_loss: 5.3712 - val_mse: 5.3712\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8884 - mse: 5.8884 - val_loss: 5.7383 - val_mse: 5.7383\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.9246 - mse: 5.9246 - val_loss: 5.5783 - val_mse: 5.5783\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.1663 - mse: 6.1663 - val_loss: 5.7890 - val_mse: 5.7890\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9672 - mse: 5.9672 - val_loss: 5.2916 - val_mse: 5.2916\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9431 - mse: 5.9431 - val_loss: 6.0478 - val_mse: 6.0478\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.9388 - mse: 5.9388 - val_loss: 5.6986 - val_mse: 5.6986\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.8367 - mse: 5.8367 - val_loss: 5.5437 - val_mse: 5.5437\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.8250 - mse: 5.8250 - val_loss: 5.5257 - val_mse: 5.5257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "outputId": "2514051c-8ded-49be-e7ca-9129d417a65d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# errors = np.arange(-5, 6)\n",
        "# n = len(errors)\n",
        "\n",
        "mse = history.history['mse']\n",
        "val_mse = history.history['val_mse']\n",
        "\n",
        "plt.plot(mse, marker='o')\n",
        "plt.plot(val_mse, marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.legend(['train', 'valid'])\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "outputId": "d91bd483-128e-44b9-dac5-39d15438c4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "history = model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 2s 18ms/step - loss: 22.2263 - mae: 22.2263 - val_loss: 20.6549 - val_mae: 20.6549\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 21.0643 - mae: 21.0643 - val_loss: 19.2188 - val_mae: 19.2188\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19.1867 - mae: 19.1867 - val_loss: 16.6551 - val_mae: 16.6551\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 15.8389 - mae: 15.8389 - val_loss: 12.3415 - val_mae: 12.3415\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.9569 - mae: 10.9569 - val_loss: 7.5466 - val_mae: 7.5466\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7.4699 - mae: 7.4699 - val_loss: 6.1388 - val_mae: 6.1388\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.1225 - mae: 6.1225 - val_loss: 4.5723 - val_mae: 4.5723\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4.7912 - mae: 4.7912 - val_loss: 3.5895 - val_mae: 3.5895\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3.8671 - mae: 3.8671 - val_loss: 3.5031 - val_mae: 3.5031\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3.4021 - mae: 3.4021 - val_loss: 3.1003 - val_mae: 3.1003\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.2021 - mae: 3.2021 - val_loss: 3.0029 - val_mae: 3.0029\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.0434 - mae: 3.0434 - val_loss: 2.7823 - val_mae: 2.7823\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.9103 - mae: 2.9103 - val_loss: 2.6655 - val_mae: 2.6655\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.8104 - mae: 2.8104 - val_loss: 2.4825 - val_mae: 2.4825\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.7283 - mae: 2.7283 - val_loss: 2.4593 - val_mae: 2.4593\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.6165 - mae: 2.6165 - val_loss: 2.4300 - val_mae: 2.4300\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.5385 - mae: 2.5385 - val_loss: 2.4039 - val_mae: 2.4039\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.4856 - mae: 2.4856 - val_loss: 2.2213 - val_mae: 2.2213\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.5163 - mae: 2.5163 - val_loss: 2.1957 - val_mae: 2.1957\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.3872 - mae: 2.3872 - val_loss: 2.1663 - val_mae: 2.1663\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.3194 - mae: 2.3194 - val_loss: 2.0607 - val_mae: 2.0607\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.2925 - mae: 2.2925 - val_loss: 2.1196 - val_mae: 2.1196\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.2480 - mae: 2.2480 - val_loss: 2.1156 - val_mae: 2.1156\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.2426 - mae: 2.2426 - val_loss: 2.0225 - val_mae: 2.0225\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.2151 - mae: 2.2151 - val_loss: 2.0736 - val_mae: 2.0736\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1847 - mae: 2.1847 - val_loss: 2.1087 - val_mae: 2.1087\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2006 - mae: 2.2006 - val_loss: 2.0816 - val_mae: 2.0816\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1346 - mae: 2.1346 - val_loss: 2.0629 - val_mae: 2.0629\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2083 - mae: 2.2083 - val_loss: 1.9943 - val_mae: 1.9943\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1243 - mae: 2.1243 - val_loss: 2.0941 - val_mae: 2.0941\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.1107 - mae: 2.1107 - val_loss: 1.9831 - val_mae: 1.9831\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.0719 - mae: 2.0719 - val_loss: 2.0927 - val_mae: 2.0927\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.0766 - mae: 2.0766 - val_loss: 2.0737 - val_mae: 2.0737\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0895 - mae: 2.0895 - val_loss: 2.0120 - val_mae: 2.0120\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.0376 - mae: 2.0376 - val_loss: 1.9657 - val_mae: 1.9657\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.0202 - mae: 2.0202 - val_loss: 2.0235 - val_mae: 2.0235\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9942 - mae: 1.9942 - val_loss: 2.0426 - val_mae: 2.0426\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.0302 - mae: 2.0302 - val_loss: 1.9321 - val_mae: 1.9321\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9699 - mae: 1.9699 - val_loss: 2.0210 - val_mae: 2.0210\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9667 - mae: 1.9667 - val_loss: 1.9785 - val_mae: 1.9785\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9639 - mae: 1.9639 - val_loss: 1.9314 - val_mae: 1.9314\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9501 - mae: 1.9501 - val_loss: 2.0174 - val_mae: 2.0174\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9448 - mae: 1.9448 - val_loss: 1.9511 - val_mae: 1.9511\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9309 - mae: 1.9309 - val_loss: 1.9930 - val_mae: 1.9930\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9538 - mae: 1.9538 - val_loss: 1.9348 - val_mae: 1.9348\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9138 - mae: 1.9138 - val_loss: 1.9820 - val_mae: 1.9820\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8867 - mae: 1.8867 - val_loss: 1.9445 - val_mae: 1.9445\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8899 - mae: 1.8899 - val_loss: 2.0206 - val_mae: 2.0206\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8752 - mae: 1.8752 - val_loss: 1.9376 - val_mae: 1.9376\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8750 - mae: 1.8750 - val_loss: 1.9999 - val_mae: 1.9999\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8651 - mae: 1.8651 - val_loss: 1.9175 - val_mae: 1.9175\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8858 - mae: 1.8858 - val_loss: 2.0677 - val_mae: 2.0677\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9101 - mae: 1.9101 - val_loss: 1.9411 - val_mae: 1.9411\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8263 - mae: 1.8263 - val_loss: 1.9956 - val_mae: 1.9956\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8279 - mae: 1.8279 - val_loss: 1.9515 - val_mae: 1.9515\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8092 - mae: 1.8092 - val_loss: 1.9473 - val_mae: 1.9473\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8221 - mae: 1.8221 - val_loss: 2.0330 - val_mae: 2.0330\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8194 - mae: 1.8194 - val_loss: 1.9552 - val_mae: 1.9552\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8147 - mae: 1.8147 - val_loss: 1.9316 - val_mae: 1.9316\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7956 - mae: 1.7956 - val_loss: 1.9791 - val_mae: 1.9791\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7757 - mae: 1.7757 - val_loss: 1.9338 - val_mae: 1.9338\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.8008 - mae: 1.8008 - val_loss: 2.0176 - val_mae: 2.0176\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7548 - mae: 1.7548 - val_loss: 1.9697 - val_mae: 1.9697\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7689 - mae: 1.7689 - val_loss: 1.9763 - val_mae: 1.9763\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7472 - mae: 1.7472 - val_loss: 1.9873 - val_mae: 1.9873\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7563 - mae: 1.7563 - val_loss: 2.0582 - val_mae: 2.0582\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7673 - mae: 1.7673 - val_loss: 1.9502 - val_mae: 1.9502\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7131 - mae: 1.7131 - val_loss: 2.0400 - val_mae: 2.0400\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7074 - mae: 1.7074 - val_loss: 2.0142 - val_mae: 2.0142\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7015 - mae: 1.7015 - val_loss: 2.0552 - val_mae: 2.0552\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7190 - mae: 1.7190 - val_loss: 2.0562 - val_mae: 2.0562\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7359 - mae: 1.7359 - val_loss: 2.0297 - val_mae: 2.0297\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6897 - mae: 1.6897 - val_loss: 2.0393 - val_mae: 2.0393\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.6878 - mae: 1.6878 - val_loss: 2.0519 - val_mae: 2.0519\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6609 - mae: 1.6609 - val_loss: 2.0543 - val_mae: 2.0543\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6544 - mae: 1.6544 - val_loss: 2.0490 - val_mae: 2.0490\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6667 - mae: 1.6667 - val_loss: 2.0878 - val_mae: 2.0878\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6696 - mae: 1.6696 - val_loss: 2.0422 - val_mae: 2.0422\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6324 - mae: 1.6324 - val_loss: 2.1001 - val_mae: 2.1001\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6470 - mae: 1.6470 - val_loss: 2.0860 - val_mae: 2.0860\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6406 - mae: 1.6406 - val_loss: 2.0888 - val_mae: 2.0888\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6205 - mae: 1.6205 - val_loss: 2.0861 - val_mae: 2.0861\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6424 - mae: 1.6424 - val_loss: 2.0595 - val_mae: 2.0595\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6141 - mae: 1.6141 - val_loss: 2.0819 - val_mae: 2.0819\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6156 - mae: 1.6156 - val_loss: 2.1178 - val_mae: 2.1178\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6414 - mae: 1.6414 - val_loss: 2.1202 - val_mae: 2.1202\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6656 - mae: 1.6656 - val_loss: 2.1022 - val_mae: 2.1022\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6226 - mae: 1.6226 - val_loss: 2.1571 - val_mae: 2.1571\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6124 - mae: 1.6124 - val_loss: 2.1551 - val_mae: 2.1551\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6093 - mae: 1.6093 - val_loss: 2.1500 - val_mae: 2.1500\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6004 - mae: 1.6004 - val_loss: 2.1171 - val_mae: 2.1171\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5852 - mae: 1.5852 - val_loss: 2.1422 - val_mae: 2.1422\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5609 - mae: 1.5609 - val_loss: 2.1249 - val_mae: 2.1249\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5527 - mae: 1.5527 - val_loss: 2.1447 - val_mae: 2.1447\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5491 - mae: 1.5491 - val_loss: 2.1363 - val_mae: 2.1363\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5478 - mae: 1.5478 - val_loss: 2.0831 - val_mae: 2.0831\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5737 - mae: 1.5737 - val_loss: 2.1257 - val_mae: 2.1257\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5353 - mae: 1.5353 - val_loss: 2.1331 - val_mae: 2.1331\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5846 - mae: 1.5846 - val_loss: 2.0618 - val_mae: 2.0618\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5750 - mae: 1.5750 - val_loss: 2.1652 - val_mae: 2.1652\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5413 - mae: 1.5413 - val_loss: 2.1132 - val_mae: 2.1132\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5750 - mae: 1.5750 - val_loss: 2.0915 - val_mae: 2.0915\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5121 - mae: 1.5121 - val_loss: 2.1728 - val_mae: 2.1728\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5623 - mae: 1.5623 - val_loss: 2.1541 - val_mae: 2.1541\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5450 - mae: 1.5450 - val_loss: 2.1251 - val_mae: 2.1251\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.5283 - mae: 1.5283 - val_loss: 2.1168 - val_mae: 2.1168\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5443 - mae: 1.5443 - val_loss: 2.0956 - val_mae: 2.0956\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6061 - mae: 1.6061 - val_loss: 2.0581 - val_mae: 2.0581\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5267 - mae: 1.5267 - val_loss: 2.0680 - val_mae: 2.0680\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4925 - mae: 1.4925 - val_loss: 2.1248 - val_mae: 2.1248\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5129 - mae: 1.5129 - val_loss: 2.1185 - val_mae: 2.1185\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4809 - mae: 1.4809 - val_loss: 2.0803 - val_mae: 2.0803\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4906 - mae: 1.4906 - val_loss: 2.1210 - val_mae: 2.1210\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4852 - mae: 1.4852 - val_loss: 2.1061 - val_mae: 2.1061\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5428 - mae: 1.5428 - val_loss: 2.0976 - val_mae: 2.0976\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4882 - mae: 1.4882 - val_loss: 2.0571 - val_mae: 2.0571\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4998 - mae: 1.4998 - val_loss: 2.0809 - val_mae: 2.0809\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5153 - mae: 1.5153 - val_loss: 2.1304 - val_mae: 2.1304\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5304 - mae: 1.5304 - val_loss: 2.0802 - val_mae: 2.0802\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4833 - mae: 1.4833 - val_loss: 2.0857 - val_mae: 2.0857\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4700 - mae: 1.4700 - val_loss: 2.0982 - val_mae: 2.0982\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4893 - mae: 1.4893 - val_loss: 2.0770 - val_mae: 2.0770\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5180 - mae: 1.5180 - val_loss: 2.1188 - val_mae: 2.1188\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5812 - mae: 1.5812 - val_loss: 2.1108 - val_mae: 2.1108\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4642 - mae: 1.4642 - val_loss: 2.0802 - val_mae: 2.0802\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4356 - mae: 1.4356 - val_loss: 2.0883 - val_mae: 2.0883\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4197 - mae: 1.4197 - val_loss: 2.1029 - val_mae: 2.1029\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4514 - mae: 1.4514 - val_loss: 2.1143 - val_mae: 2.1143\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4858 - mae: 1.4858 - val_loss: 2.1731 - val_mae: 2.1731\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4752 - mae: 1.4752 - val_loss: 2.1140 - val_mae: 2.1140\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4538 - mae: 1.4538 - val_loss: 2.0482 - val_mae: 2.0482\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4813 - mae: 1.4813 - val_loss: 2.1172 - val_mae: 2.1172\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4922 - mae: 1.4922 - val_loss: 2.0940 - val_mae: 2.0940\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4260 - mae: 1.4260 - val_loss: 2.1240 - val_mae: 2.1240\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4497 - mae: 1.4497 - val_loss: 2.1576 - val_mae: 2.1576\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4836 - mae: 1.4836 - val_loss: 2.0959 - val_mae: 2.0959\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4472 - mae: 1.4472 - val_loss: 2.0935 - val_mae: 2.0935\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4967 - mae: 1.4967 - val_loss: 2.0908 - val_mae: 2.0908\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4699 - mae: 1.4699 - val_loss: 2.0625 - val_mae: 2.0625\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4283 - mae: 1.4283 - val_loss: 2.0835 - val_mae: 2.0835\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4169 - mae: 1.4169 - val_loss: 2.1021 - val_mae: 2.1021\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4304 - mae: 1.4304 - val_loss: 2.0765 - val_mae: 2.0765\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4572 - mae: 1.4572 - val_loss: 2.0375 - val_mae: 2.0375\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4081 - mae: 1.4081 - val_loss: 2.1139 - val_mae: 2.1139\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4498 - mae: 1.4498 - val_loss: 2.2047 - val_mae: 2.2047\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5450 - mae: 1.5450 - val_loss: 2.1040 - val_mae: 2.1040\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5168 - mae: 1.5168 - val_loss: 2.1120 - val_mae: 2.1120\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4228 - mae: 1.4228 - val_loss: 2.0445 - val_mae: 2.0445\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.4020 - mae: 1.4020 - val_loss: 2.0841 - val_mae: 2.0841\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4454 - mae: 1.4454 - val_loss: 2.0753 - val_mae: 2.0753\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3973 - mae: 1.3973 - val_loss: 2.0777 - val_mae: 2.0777\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4190 - mae: 1.4190 - val_loss: 2.0503 - val_mae: 2.0503\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.4230 - mae: 1.4230 - val_loss: 2.0695 - val_mae: 2.0695\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3818 - mae: 1.3818 - val_loss: 2.0823 - val_mae: 2.0823\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3807 - mae: 1.3807 - val_loss: 2.0691 - val_mae: 2.0691\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4000 - mae: 1.4000 - val_loss: 2.0206 - val_mae: 2.0206\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.3868 - mae: 1.3868 - val_loss: 2.0927 - val_mae: 2.0927\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.3848 - mae: 1.3848 - val_loss: 2.0874 - val_mae: 2.0874\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3903 - mae: 1.3903 - val_loss: 2.0769 - val_mae: 2.0769\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3840 - mae: 1.3840 - val_loss: 2.0544 - val_mae: 2.0544\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3681 - mae: 1.3681 - val_loss: 2.0822 - val_mae: 2.0822\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3627 - mae: 1.3627 - val_loss: 2.0677 - val_mae: 2.0677\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3588 - mae: 1.3588 - val_loss: 2.0348 - val_mae: 2.0348\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.3590 - mae: 1.3590 - val_loss: 2.0798 - val_mae: 2.0798\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3586 - mae: 1.3586 - val_loss: 2.0570 - val_mae: 2.0570\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3987 - mae: 1.3987 - val_loss: 2.0645 - val_mae: 2.0645\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.3987 - mae: 1.3987 - val_loss: 2.0741 - val_mae: 2.0741\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3959 - mae: 1.3959 - val_loss: 2.0484 - val_mae: 2.0484\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3481 - mae: 1.3481 - val_loss: 2.0376 - val_mae: 2.0376\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3451 - mae: 1.3451 - val_loss: 2.0802 - val_mae: 2.0802\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3763 - mae: 1.3763 - val_loss: 2.0609 - val_mae: 2.0609\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3443 - mae: 1.3443 - val_loss: 1.9978 - val_mae: 1.9978\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3587 - mae: 1.3587 - val_loss: 2.0326 - val_mae: 2.0326\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3527 - mae: 1.3527 - val_loss: 2.0854 - val_mae: 2.0854\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3865 - mae: 1.3865 - val_loss: 2.0475 - val_mae: 2.0475\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3816 - mae: 1.3816 - val_loss: 2.0446 - val_mae: 2.0446\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3838 - mae: 1.3838 - val_loss: 2.0196 - val_mae: 2.0196\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3466 - mae: 1.3466 - val_loss: 2.0877 - val_mae: 2.0877\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3388 - mae: 1.3388 - val_loss: 2.0499 - val_mae: 2.0499\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3130 - mae: 1.3130 - val_loss: 2.0781 - val_mae: 2.0781\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3188 - mae: 1.3188 - val_loss: 2.0419 - val_mae: 2.0419\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2987 - mae: 1.2987 - val_loss: 2.0302 - val_mae: 2.0302\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3913 - mae: 1.3913 - val_loss: 2.0652 - val_mae: 2.0652\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3515 - mae: 1.3515 - val_loss: 2.0652 - val_mae: 2.0652\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3093 - mae: 1.3093 - val_loss: 2.0316 - val_mae: 2.0316\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3338 - mae: 1.3338 - val_loss: 2.0021 - val_mae: 2.0021\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2816 - mae: 1.2816 - val_loss: 2.0432 - val_mae: 2.0432\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3601 - mae: 1.3601 - val_loss: 1.9908 - val_mae: 1.9908\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3560 - mae: 1.3560 - val_loss: 2.0213 - val_mae: 2.0213\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3208 - mae: 1.3208 - val_loss: 2.0422 - val_mae: 2.0422\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3374 - mae: 1.3374 - val_loss: 2.0199 - val_mae: 2.0199\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2829 - mae: 1.2829 - val_loss: 2.0440 - val_mae: 2.0440\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3019 - mae: 1.3019 - val_loss: 2.0478 - val_mae: 2.0478\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3167 - mae: 1.3167 - val_loss: 2.1037 - val_mae: 2.1037\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3195 - mae: 1.3195 - val_loss: 2.0208 - val_mae: 2.0208\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3297 - mae: 1.3297 - val_loss: 2.0351 - val_mae: 2.0351\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3098 - mae: 1.3098 - val_loss: 2.0012 - val_mae: 2.0012\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2998 - mae: 1.2998 - val_loss: 2.0173 - val_mae: 2.0173\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2888 - mae: 1.2888 - val_loss: 2.0256 - val_mae: 2.0256\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2759 - mae: 1.2759 - val_loss: 1.9947 - val_mae: 1.9947\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2770 - mae: 1.2770 - val_loss: 2.0340 - val_mae: 2.0340\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2913 - mae: 1.2913 - val_loss: 2.0688 - val_mae: 2.0688\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2984 - mae: 1.2984 - val_loss: 2.0588 - val_mae: 2.0588\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2594 - mae: 1.2594 - val_loss: 1.9753 - val_mae: 1.9753\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2670 - mae: 1.2670 - val_loss: 2.0068 - val_mae: 2.0068\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2575 - mae: 1.2575 - val_loss: 1.9806 - val_mae: 1.9806\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2505 - mae: 1.2505 - val_loss: 2.0428 - val_mae: 2.0428\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2748 - mae: 1.2748 - val_loss: 2.0407 - val_mae: 2.0407\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3137 - mae: 1.3137 - val_loss: 1.9854 - val_mae: 1.9854\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3359 - mae: 1.3359 - val_loss: 1.9645 - val_mae: 1.9645\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2768 - mae: 1.2768 - val_loss: 2.0235 - val_mae: 2.0235\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2489 - mae: 1.2489 - val_loss: 2.0215 - val_mae: 2.0215\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2432 - mae: 1.2432 - val_loss: 2.0101 - val_mae: 2.0101\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2590 - mae: 1.2590 - val_loss: 1.9862 - val_mae: 1.9862\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2367 - mae: 1.2367 - val_loss: 2.0018 - val_mae: 2.0018\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2800 - mae: 1.2800 - val_loss: 1.9992 - val_mae: 1.9992\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3255 - mae: 1.3255 - val_loss: 2.1230 - val_mae: 2.1230\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3321 - mae: 1.3321 - val_loss: 2.0096 - val_mae: 2.0096\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2858 - mae: 1.2858 - val_loss: 2.0049 - val_mae: 2.0049\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2801 - mae: 1.2801 - val_loss: 2.0541 - val_mae: 2.0541\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3274 - mae: 1.3274 - val_loss: 2.0448 - val_mae: 2.0448\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3052 - mae: 1.3052 - val_loss: 2.0089 - val_mae: 2.0089\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2637 - mae: 1.2637 - val_loss: 1.9726 - val_mae: 1.9726\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2884 - mae: 1.2884 - val_loss: 1.9923 - val_mae: 1.9923\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2372 - mae: 1.2372 - val_loss: 2.0602 - val_mae: 2.0602\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2748 - mae: 1.2748 - val_loss: 2.0100 - val_mae: 2.0100\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2488 - mae: 1.2488 - val_loss: 2.0141 - val_mae: 2.0141\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2813 - mae: 1.2813 - val_loss: 1.9773 - val_mae: 1.9773\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2317 - mae: 1.2317 - val_loss: 2.0014 - val_mae: 2.0014\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2325 - mae: 1.2325 - val_loss: 1.9721 - val_mae: 1.9721\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2110 - mae: 1.2110 - val_loss: 1.9586 - val_mae: 1.9586\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2396 - mae: 1.2396 - val_loss: 1.9796 - val_mae: 1.9796\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2550 - mae: 1.2550 - val_loss: 2.0304 - val_mae: 2.0304\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3035 - mae: 1.3035 - val_loss: 1.9895 - val_mae: 1.9895\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2607 - mae: 1.2607 - val_loss: 1.9621 - val_mae: 1.9621\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2597 - mae: 1.2597 - val_loss: 2.0175 - val_mae: 2.0175\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2578 - mae: 1.2578 - val_loss: 1.9836 - val_mae: 1.9836\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2862 - mae: 1.2862 - val_loss: 1.9246 - val_mae: 1.9246\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2543 - mae: 1.2543 - val_loss: 1.9533 - val_mae: 1.9533\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2321 - mae: 1.2321 - val_loss: 1.9467 - val_mae: 1.9467\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2341 - mae: 1.2341 - val_loss: 1.9199 - val_mae: 1.9199\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2384 - mae: 1.2384 - val_loss: 1.9508 - val_mae: 1.9508\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2923 - mae: 1.2923 - val_loss: 1.9626 - val_mae: 1.9626\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2515 - mae: 1.2515 - val_loss: 1.9463 - val_mae: 1.9463\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2084 - mae: 1.2084 - val_loss: 1.9353 - val_mae: 1.9353\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1961 - mae: 1.1961 - val_loss: 1.8897 - val_mae: 1.8897\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2299 - mae: 1.2299 - val_loss: 1.9394 - val_mae: 1.9394\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1986 - mae: 1.1986 - val_loss: 1.9135 - val_mae: 1.9135\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1928 - mae: 1.1928 - val_loss: 1.9126 - val_mae: 1.9126\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2137 - mae: 1.2137 - val_loss: 1.8948 - val_mae: 1.8948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "outputId": "e20eca63-632c-4afb-a172-d9533e227dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# errors = np.arange(-5, 6)\n",
        "# n = len(errors)\n",
        "\n",
        "mae =  history.history['mae']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "plt.plot(mae, marker='o')\n",
        "plt.plot(val_mae, marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.legend(['train', 'valid'])\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "outputId": "9b4ea9b4-effe-415e-b688-3bd850e53c97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "history = model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 2s 44ms/step - loss: 8.2880 - mean_squared_logarithmic_error: 8.2880 - val_loss: 5.9091 - val_mean_squared_logarithmic_error: 5.9091\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.0006 - mean_squared_logarithmic_error: 5.0006 - val_loss: 3.5628 - val_mean_squared_logarithmic_error: 3.5628\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.0310 - mean_squared_logarithmic_error: 3.0310 - val_loss: 2.0387 - val_mean_squared_logarithmic_error: 2.0387\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.7071 - mean_squared_logarithmic_error: 1.7071 - val_loss: 1.0785 - val_mean_squared_logarithmic_error: 1.0785\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.9112 - mean_squared_logarithmic_error: 0.9112 - val_loss: 0.5487 - val_mean_squared_logarithmic_error: 0.5487\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4821 - mean_squared_logarithmic_error: 0.4821 - val_loss: 0.2830 - val_mean_squared_logarithmic_error: 0.2830\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2752 - mean_squared_logarithmic_error: 0.2752 - val_loss: 0.1612 - val_mean_squared_logarithmic_error: 0.1612\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1729 - mean_squared_logarithmic_error: 0.1729 - val_loss: 0.1112 - val_mean_squared_logarithmic_error: 0.1112\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1284 - mean_squared_logarithmic_error: 0.1284 - val_loss: 0.0901 - val_mean_squared_logarithmic_error: 0.0901\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1088 - mean_squared_logarithmic_error: 0.1088 - val_loss: 0.0788 - val_mean_squared_logarithmic_error: 0.0788\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0956 - mean_squared_logarithmic_error: 0.0956 - val_loss: 0.0717 - val_mean_squared_logarithmic_error: 0.0717\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0874 - mean_squared_logarithmic_error: 0.0874 - val_loss: 0.0653 - val_mean_squared_logarithmic_error: 0.0653\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0797 - mean_squared_logarithmic_error: 0.0797 - val_loss: 0.0597 - val_mean_squared_logarithmic_error: 0.0597\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0735 - mean_squared_logarithmic_error: 0.0735 - val_loss: 0.0544 - val_mean_squared_logarithmic_error: 0.0544\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0681 - mean_squared_logarithmic_error: 0.0681 - val_loss: 0.0504 - val_mean_squared_logarithmic_error: 0.0504\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0633 - mean_squared_logarithmic_error: 0.0633 - val_loss: 0.0468 - val_mean_squared_logarithmic_error: 0.0468\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0592 - mean_squared_logarithmic_error: 0.0592 - val_loss: 0.0436 - val_mean_squared_logarithmic_error: 0.0436\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0556 - mean_squared_logarithmic_error: 0.0556 - val_loss: 0.0405 - val_mean_squared_logarithmic_error: 0.0405\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0523 - mean_squared_logarithmic_error: 0.0523 - val_loss: 0.0380 - val_mean_squared_logarithmic_error: 0.0380\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0494 - mean_squared_logarithmic_error: 0.0494 - val_loss: 0.0358 - val_mean_squared_logarithmic_error: 0.0358\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0469 - mean_squared_logarithmic_error: 0.0469 - val_loss: 0.0342 - val_mean_squared_logarithmic_error: 0.0342\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0447 - mean_squared_logarithmic_error: 0.0447 - val_loss: 0.0327 - val_mean_squared_logarithmic_error: 0.0327\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0425 - mean_squared_logarithmic_error: 0.0425 - val_loss: 0.0313 - val_mean_squared_logarithmic_error: 0.0313\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0407 - mean_squared_logarithmic_error: 0.0407 - val_loss: 0.0301 - val_mean_squared_logarithmic_error: 0.0301\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0391 - mean_squared_logarithmic_error: 0.0391 - val_loss: 0.0291 - val_mean_squared_logarithmic_error: 0.0291\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0376 - mean_squared_logarithmic_error: 0.0376 - val_loss: 0.0283 - val_mean_squared_logarithmic_error: 0.0283\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0365 - mean_squared_logarithmic_error: 0.0365 - val_loss: 0.0278 - val_mean_squared_logarithmic_error: 0.0278\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0352 - mean_squared_logarithmic_error: 0.0352 - val_loss: 0.0272 - val_mean_squared_logarithmic_error: 0.0272\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0342 - mean_squared_logarithmic_error: 0.0342 - val_loss: 0.0269 - val_mean_squared_logarithmic_error: 0.0269\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0334 - mean_squared_logarithmic_error: 0.0334 - val_loss: 0.0267 - val_mean_squared_logarithmic_error: 0.0267\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0325 - mean_squared_logarithmic_error: 0.0325 - val_loss: 0.0261 - val_mean_squared_logarithmic_error: 0.0261\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0317 - mean_squared_logarithmic_error: 0.0317 - val_loss: 0.0257 - val_mean_squared_logarithmic_error: 0.0257\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0311 - mean_squared_logarithmic_error: 0.0311 - val_loss: 0.0256 - val_mean_squared_logarithmic_error: 0.0256\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0306 - mean_squared_logarithmic_error: 0.0306 - val_loss: 0.0256 - val_mean_squared_logarithmic_error: 0.0256\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0299 - mean_squared_logarithmic_error: 0.0299 - val_loss: 0.0251 - val_mean_squared_logarithmic_error: 0.0251\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0293 - mean_squared_logarithmic_error: 0.0293 - val_loss: 0.0249 - val_mean_squared_logarithmic_error: 0.0249\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0288 - mean_squared_logarithmic_error: 0.0288 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0284 - mean_squared_logarithmic_error: 0.0284 - val_loss: 0.0244 - val_mean_squared_logarithmic_error: 0.0244\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0279 - mean_squared_logarithmic_error: 0.0279 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0275 - mean_squared_logarithmic_error: 0.0275 - val_loss: 0.0244 - val_mean_squared_logarithmic_error: 0.0244\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0271 - mean_squared_logarithmic_error: 0.0271 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0268 - mean_squared_logarithmic_error: 0.0268 - val_loss: 0.0241 - val_mean_squared_logarithmic_error: 0.0241\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0264 - mean_squared_logarithmic_error: 0.0264 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0260 - mean_squared_logarithmic_error: 0.0260 - val_loss: 0.0235 - val_mean_squared_logarithmic_error: 0.0235\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0257 - mean_squared_logarithmic_error: 0.0257 - val_loss: 0.0232 - val_mean_squared_logarithmic_error: 0.0232\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0254 - mean_squared_logarithmic_error: 0.0254 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0251 - mean_squared_logarithmic_error: 0.0251 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0249 - mean_squared_logarithmic_error: 0.0249 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0245 - mean_squared_logarithmic_error: 0.0245 - val_loss: 0.0229 - val_mean_squared_logarithmic_error: 0.0229\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0242 - mean_squared_logarithmic_error: 0.0242 - val_loss: 0.0230 - val_mean_squared_logarithmic_error: 0.0230\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0240 - mean_squared_logarithmic_error: 0.0240 - val_loss: 0.0229 - val_mean_squared_logarithmic_error: 0.0229\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0237 - mean_squared_logarithmic_error: 0.0237 - val_loss: 0.0226 - val_mean_squared_logarithmic_error: 0.0226\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0235 - mean_squared_logarithmic_error: 0.0235 - val_loss: 0.0222 - val_mean_squared_logarithmic_error: 0.0222\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0232 - mean_squared_logarithmic_error: 0.0232 - val_loss: 0.0222 - val_mean_squared_logarithmic_error: 0.0222\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0230 - mean_squared_logarithmic_error: 0.0230 - val_loss: 0.0221 - val_mean_squared_logarithmic_error: 0.0221\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0228 - mean_squared_logarithmic_error: 0.0228 - val_loss: 0.0222 - val_mean_squared_logarithmic_error: 0.0222\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0226 - mean_squared_logarithmic_error: 0.0226 - val_loss: 0.0224 - val_mean_squared_logarithmic_error: 0.0224\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0224 - mean_squared_logarithmic_error: 0.0224 - val_loss: 0.0222 - val_mean_squared_logarithmic_error: 0.0222\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0222 - mean_squared_logarithmic_error: 0.0222 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0218 - mean_squared_logarithmic_error: 0.0218 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0218 - mean_squared_logarithmic_error: 0.0218 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0216 - mean_squared_logarithmic_error: 0.0216 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0215 - mean_squared_logarithmic_error: 0.0215 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.0211 - mean_squared_logarithmic_error: 0.0211 - val_loss: 0.0217 - val_mean_squared_logarithmic_error: 0.0217\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.0208 - mean_squared_logarithmic_error: 0.0208 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0207 - mean_squared_logarithmic_error: 0.0207 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0204 - mean_squared_logarithmic_error: 0.0204 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0203 - mean_squared_logarithmic_error: 0.0203 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0201 - mean_squared_logarithmic_error: 0.0201 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0200 - mean_squared_logarithmic_error: 0.0200 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0197 - mean_squared_logarithmic_error: 0.0197 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0196 - mean_squared_logarithmic_error: 0.0196 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0192 - mean_squared_logarithmic_error: 0.0192 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0192 - mean_squared_logarithmic_error: 0.0192 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0190 - mean_squared_logarithmic_error: 0.0190 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0189 - mean_squared_logarithmic_error: 0.0189 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0186 - mean_squared_logarithmic_error: 0.0186 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0169 - mean_squared_logarithmic_error: 0.0169 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0154 - mean_squared_logarithmic_error: 0.0154 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0155 - mean_squared_logarithmic_error: 0.0155 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 41ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.0149 - mean_squared_logarithmic_error: 0.0149 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.0147 - mean_squared_logarithmic_error: 0.0147 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0146 - mean_squared_logarithmic_error: 0.0146 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.0147 - mean_squared_logarithmic_error: 0.0147 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0151 - mean_squared_logarithmic_error: 0.0151 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0147 - mean_squared_logarithmic_error: 0.0147 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0144 - mean_squared_logarithmic_error: 0.0144 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0141 - mean_squared_logarithmic_error: 0.0141 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0141 - mean_squared_logarithmic_error: 0.0141 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0143 - mean_squared_logarithmic_error: 0.0143 - val_loss: 0.0169 - val_mean_squared_logarithmic_error: 0.0169\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - mean_squared_logarithmic_error: 0.0139 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - mean_squared_logarithmic_error: 0.0139 - val_loss: 0.0171 - val_mean_squared_logarithmic_error: 0.0171\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - mean_squared_logarithmic_error: 0.0137 - val_loss: 0.0166 - val_mean_squared_logarithmic_error: 0.0166\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0134 - mean_squared_logarithmic_error: 0.0134 - val_loss: 0.0166 - val_mean_squared_logarithmic_error: 0.0166\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0134 - mean_squared_logarithmic_error: 0.0134 - val_loss: 0.0164 - val_mean_squared_logarithmic_error: 0.0164\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0138 - mean_squared_logarithmic_error: 0.0138 - val_loss: 0.0168 - val_mean_squared_logarithmic_error: 0.0168\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0134 - mean_squared_logarithmic_error: 0.0134 - val_loss: 0.0164 - val_mean_squared_logarithmic_error: 0.0164\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0135 - mean_squared_logarithmic_error: 0.0135 - val_loss: 0.0168 - val_mean_squared_logarithmic_error: 0.0168\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - mean_squared_logarithmic_error: 0.0133 - val_loss: 0.0160 - val_mean_squared_logarithmic_error: 0.0160\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0131 - mean_squared_logarithmic_error: 0.0131 - val_loss: 0.0165 - val_mean_squared_logarithmic_error: 0.0165\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0130 - mean_squared_logarithmic_error: 0.0130 - val_loss: 0.0162 - val_mean_squared_logarithmic_error: 0.0162\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0127 - mean_squared_logarithmic_error: 0.0127 - val_loss: 0.0163 - val_mean_squared_logarithmic_error: 0.0163\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - mean_squared_logarithmic_error: 0.0128 - val_loss: 0.0163 - val_mean_squared_logarithmic_error: 0.0163\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0125 - mean_squared_logarithmic_error: 0.0125 - val_loss: 0.0157 - val_mean_squared_logarithmic_error: 0.0157\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0125 - mean_squared_logarithmic_error: 0.0125 - val_loss: 0.0157 - val_mean_squared_logarithmic_error: 0.0157\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0124 - mean_squared_logarithmic_error: 0.0124 - val_loss: 0.0161 - val_mean_squared_logarithmic_error: 0.0161\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - mean_squared_logarithmic_error: 0.0124 - val_loss: 0.0164 - val_mean_squared_logarithmic_error: 0.0164\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - mean_squared_logarithmic_error: 0.0122 - val_loss: 0.0157 - val_mean_squared_logarithmic_error: 0.0157\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - mean_squared_logarithmic_error: 0.0122 - val_loss: 0.0156 - val_mean_squared_logarithmic_error: 0.0156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-",
        "outputId": "c29b117c-1193-4731-b531-41acf62430f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# actual_outputs = np.arange(0, 51)\n",
        "# n = len(actual_outputs)\n",
        "# estimated_outputs = np.zeros(n)\n",
        "\n",
        "msle = history.history['mean_squared_logarithmic_error']\n",
        "val_msle = history.history['val_mean_squared_logarithmic_error']\n",
        "\n",
        "plt.plot(msle, marker='o')\n",
        "plt.plot(val_msle, marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual ouputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.legend(['train', 'valid'])\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX5w42vRXMWj"
      },
      "source": [
        "### Answer \n",
        "\n",
        "In mean squared logarithmic error (MSLE), adding 1 to the outputs is a common practice in many regression problems, particularly when the predicted values can be very small. By adding 1 to the predicted values, the logarithmic error is penalized less when the predictions are close to 0 and more when the predictions are far from 0. This helps to address the issue of penalizing large errors more than small errors, which is a common issue with mean squared error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOUf7iOkYxcM"
      },
      "source": [
        "### Answer 5\n",
        "\n",
        "\n",
        "MSE, MAE, and MSLE are all commonly used loss functions in regression problems. Each of these loss functions have different characteristics that make them appropriate for different use cases.\n",
        "\n",
        "Mean Squared Error (MSE) is a widely used loss function that measures the average squared difference between the predicted and actual values. MSE penalizes large errors more than small errors, which makes it sensitive to outliers. MSE is widely used in regression problems where it is important to penalize large errors.\n",
        "\n",
        "Mean Absolute Error (MAE) is another commonly used loss function that measures the average absolute difference between the predicted and actual values. Unlike MSE, MAE does not penalize large errors more than small errors. This makes MAE less sensitive to outliers compared to MSE. MAE is often used in regression problems where it is important to give equal weight to both large and small errors.\n",
        "\n",
        "Mean Squared Logarithmic Error (MSLE) is a loss function that measures the average squared difference between the logarithms of the predicted and actual values. MSLE is similar to MSE in that it penalizes large errors more than small errors, but the difference is that MSLE penalizes the logarithmic difference between the predicted and actual values instead of the absolute difference. MSLE is often used in regression problems where the target values can be very small, as it is more appropriate in such cases compared to MSE or MAE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_9nfLOffJ_P"
      },
      "source": [
        "### Answer 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "outputId": "6e8e52b0-7b63-4ad7-8cb7-886a6de00437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= 'mean_absolute_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 17ms/step - loss: 22.2456 - mean_absolute_error: 22.2456 - val_loss: 20.7166 - val_mean_absolute_error: 20.7166\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 21.1534 - mean_absolute_error: 21.1534 - val_loss: 19.2663 - val_mean_absolute_error: 19.2663\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 19.3081 - mean_absolute_error: 19.3081 - val_loss: 16.7065 - val_mean_absolute_error: 16.7065\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16.0098 - mean_absolute_error: 16.0098 - val_loss: 12.3405 - val_mean_absolute_error: 12.3405\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.9836 - mean_absolute_error: 10.9836 - val_loss: 6.8050 - val_mean_absolute_error: 6.8050\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.8302 - mean_absolute_error: 6.8302 - val_loss: 5.7848 - val_mean_absolute_error: 5.7848\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.4120 - mean_absolute_error: 5.4120 - val_loss: 4.1488 - val_mean_absolute_error: 4.1488\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.1970 - mean_absolute_error: 4.1970 - val_loss: 3.3833 - val_mean_absolute_error: 3.3833\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3.4826 - mean_absolute_error: 3.4826 - val_loss: 3.4034 - val_mean_absolute_error: 3.4034\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3.1905 - mean_absolute_error: 3.1905 - val_loss: 3.1496 - val_mean_absolute_error: 3.1496\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.0153 - mean_absolute_error: 3.0153 - val_loss: 3.1631 - val_mean_absolute_error: 3.1631\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.9198 - mean_absolute_error: 2.9198 - val_loss: 2.9293 - val_mean_absolute_error: 2.9293\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.7719 - mean_absolute_error: 2.7719 - val_loss: 2.8557 - val_mean_absolute_error: 2.8557\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.6647 - mean_absolute_error: 2.6647 - val_loss: 2.6516 - val_mean_absolute_error: 2.6516\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.6040 - mean_absolute_error: 2.6040 - val_loss: 2.7524 - val_mean_absolute_error: 2.7524\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.4852 - mean_absolute_error: 2.4852 - val_loss: 2.5979 - val_mean_absolute_error: 2.5979\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.4244 - mean_absolute_error: 2.4244 - val_loss: 2.5592 - val_mean_absolute_error: 2.5592\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.3667 - mean_absolute_error: 2.3667 - val_loss: 2.4419 - val_mean_absolute_error: 2.4419\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.3451 - mean_absolute_error: 2.3451 - val_loss: 2.5476 - val_mean_absolute_error: 2.5476\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.2689 - mean_absolute_error: 2.2689 - val_loss: 2.3498 - val_mean_absolute_error: 2.3498\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.2384 - mean_absolute_error: 2.2384 - val_loss: 2.4004 - val_mean_absolute_error: 2.4004\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.2213 - mean_absolute_error: 2.2213 - val_loss: 2.4236 - val_mean_absolute_error: 2.4236\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1878 - mean_absolute_error: 2.1878 - val_loss: 2.4479 - val_mean_absolute_error: 2.4479\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1929 - mean_absolute_error: 2.1929 - val_loss: 2.3959 - val_mean_absolute_error: 2.3959\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.1992 - mean_absolute_error: 2.1992 - val_loss: 2.3377 - val_mean_absolute_error: 2.3377\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.1695 - mean_absolute_error: 2.1695 - val_loss: 2.3495 - val_mean_absolute_error: 2.3495\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.1189 - mean_absolute_error: 2.1189 - val_loss: 2.3147 - val_mean_absolute_error: 2.3147\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.0930 - mean_absolute_error: 2.0930 - val_loss: 2.3742 - val_mean_absolute_error: 2.3742\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.0700 - mean_absolute_error: 2.0700 - val_loss: 2.2614 - val_mean_absolute_error: 2.2614\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.0611 - mean_absolute_error: 2.0611 - val_loss: 2.3150 - val_mean_absolute_error: 2.3150\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.0558 - mean_absolute_error: 2.0558 - val_loss: 2.4118 - val_mean_absolute_error: 2.4118\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.0734 - mean_absolute_error: 2.0734 - val_loss: 2.2612 - val_mean_absolute_error: 2.2612\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.0456 - mean_absolute_error: 2.0456 - val_loss: 2.2735 - val_mean_absolute_error: 2.2735\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.9986 - mean_absolute_error: 1.9986 - val_loss: 2.2761 - val_mean_absolute_error: 2.2761\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.9788 - mean_absolute_error: 1.9788 - val_loss: 2.2631 - val_mean_absolute_error: 2.2631\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.9554 - mean_absolute_error: 1.9554 - val_loss: 2.2235 - val_mean_absolute_error: 2.2235\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9462 - mean_absolute_error: 1.9462 - val_loss: 2.1795 - val_mean_absolute_error: 2.1795\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.9426 - mean_absolute_error: 1.9426 - val_loss: 2.1669 - val_mean_absolute_error: 2.1669\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9346 - mean_absolute_error: 1.9346 - val_loss: 2.1881 - val_mean_absolute_error: 2.1881\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.9142 - mean_absolute_error: 1.9142 - val_loss: 2.1936 - val_mean_absolute_error: 2.1936\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.9167 - mean_absolute_error: 1.9167 - val_loss: 2.1149 - val_mean_absolute_error: 2.1149\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.8845 - mean_absolute_error: 1.8845 - val_loss: 2.2038 - val_mean_absolute_error: 2.2038\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9100 - mean_absolute_error: 1.9100 - val_loss: 2.1690 - val_mean_absolute_error: 2.1690\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.8665 - mean_absolute_error: 1.8665 - val_loss: 2.1377 - val_mean_absolute_error: 2.1377\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8954 - mean_absolute_error: 1.8954 - val_loss: 2.1358 - val_mean_absolute_error: 2.1358\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8126 - mean_absolute_error: 1.8126 - val_loss: 2.2387 - val_mean_absolute_error: 2.2387\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8847 - mean_absolute_error: 1.8847 - val_loss: 2.0829 - val_mean_absolute_error: 2.0829\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8110 - mean_absolute_error: 1.8110 - val_loss: 2.2335 - val_mean_absolute_error: 2.2335\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8482 - mean_absolute_error: 1.8482 - val_loss: 2.1311 - val_mean_absolute_error: 2.1311\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7852 - mean_absolute_error: 1.7852 - val_loss: 2.1050 - val_mean_absolute_error: 2.1050\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7513 - mean_absolute_error: 1.7513 - val_loss: 2.0364 - val_mean_absolute_error: 2.0364\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7736 - mean_absolute_error: 1.7736 - val_loss: 2.1309 - val_mean_absolute_error: 2.1309\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7942 - mean_absolute_error: 1.7942 - val_loss: 2.1171 - val_mean_absolute_error: 2.1171\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7359 - mean_absolute_error: 1.7359 - val_loss: 2.1124 - val_mean_absolute_error: 2.1124\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7544 - mean_absolute_error: 1.7544 - val_loss: 2.0786 - val_mean_absolute_error: 2.0786\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7413 - mean_absolute_error: 1.7413 - val_loss: 2.1596 - val_mean_absolute_error: 2.1596\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7205 - mean_absolute_error: 1.7205 - val_loss: 2.1481 - val_mean_absolute_error: 2.1481\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8171 - mean_absolute_error: 1.8171 - val_loss: 2.2052 - val_mean_absolute_error: 2.2052\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7783 - mean_absolute_error: 1.7783 - val_loss: 2.1589 - val_mean_absolute_error: 2.1589\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7443 - mean_absolute_error: 1.7443 - val_loss: 2.0122 - val_mean_absolute_error: 2.0122\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6716 - mean_absolute_error: 1.6716 - val_loss: 2.1886 - val_mean_absolute_error: 2.1886\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6804 - mean_absolute_error: 1.6804 - val_loss: 2.0060 - val_mean_absolute_error: 2.0060\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6815 - mean_absolute_error: 1.6815 - val_loss: 2.0690 - val_mean_absolute_error: 2.0690\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6378 - mean_absolute_error: 1.6378 - val_loss: 2.0404 - val_mean_absolute_error: 2.0404\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6358 - mean_absolute_error: 1.6358 - val_loss: 2.0817 - val_mean_absolute_error: 2.0817\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6295 - mean_absolute_error: 1.6295 - val_loss: 2.0914 - val_mean_absolute_error: 2.0914\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6699 - mean_absolute_error: 1.6699 - val_loss: 2.0415 - val_mean_absolute_error: 2.0415\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6132 - mean_absolute_error: 1.6132 - val_loss: 2.0445 - val_mean_absolute_error: 2.0445\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6637 - mean_absolute_error: 1.6637 - val_loss: 2.1003 - val_mean_absolute_error: 2.1003\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6079 - mean_absolute_error: 1.6079 - val_loss: 2.0485 - val_mean_absolute_error: 2.0485\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5715 - mean_absolute_error: 1.5715 - val_loss: 2.0633 - val_mean_absolute_error: 2.0633\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5968 - mean_absolute_error: 1.5968 - val_loss: 1.9666 - val_mean_absolute_error: 1.9666\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5749 - mean_absolute_error: 1.5749 - val_loss: 2.0331 - val_mean_absolute_error: 2.0331\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5721 - mean_absolute_error: 1.5721 - val_loss: 2.1305 - val_mean_absolute_error: 2.1305\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5978 - mean_absolute_error: 1.5978 - val_loss: 2.0121 - val_mean_absolute_error: 2.0121\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6043 - mean_absolute_error: 1.6043 - val_loss: 2.3770 - val_mean_absolute_error: 2.3770\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7364 - mean_absolute_error: 1.7364 - val_loss: 2.0111 - val_mean_absolute_error: 2.0111\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5628 - mean_absolute_error: 1.5628 - val_loss: 2.0061 - val_mean_absolute_error: 2.0061\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5421 - mean_absolute_error: 1.5421 - val_loss: 2.1336 - val_mean_absolute_error: 2.1336\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5513 - mean_absolute_error: 1.5513 - val_loss: 1.9920 - val_mean_absolute_error: 1.9920\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5394 - mean_absolute_error: 1.5394 - val_loss: 2.0399 - val_mean_absolute_error: 2.0399\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5373 - mean_absolute_error: 1.5373 - val_loss: 2.0638 - val_mean_absolute_error: 2.0638\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5259 - mean_absolute_error: 1.5259 - val_loss: 2.0345 - val_mean_absolute_error: 2.0345\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5067 - mean_absolute_error: 1.5067 - val_loss: 2.1249 - val_mean_absolute_error: 2.1249\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5511 - mean_absolute_error: 1.5511 - val_loss: 2.1203 - val_mean_absolute_error: 2.1203\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5096 - mean_absolute_error: 1.5096 - val_loss: 2.0285 - val_mean_absolute_error: 2.0285\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5200 - mean_absolute_error: 1.5200 - val_loss: 2.0475 - val_mean_absolute_error: 2.0475\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4780 - mean_absolute_error: 1.4780 - val_loss: 1.9993 - val_mean_absolute_error: 1.9993\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4701 - mean_absolute_error: 1.4701 - val_loss: 1.9865 - val_mean_absolute_error: 1.9865\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4624 - mean_absolute_error: 1.4624 - val_loss: 2.0316 - val_mean_absolute_error: 2.0316\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5115 - mean_absolute_error: 1.5115 - val_loss: 2.0498 - val_mean_absolute_error: 2.0498\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5209 - mean_absolute_error: 1.5209 - val_loss: 2.0770 - val_mean_absolute_error: 2.0770\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5503 - mean_absolute_error: 1.5503 - val_loss: 1.9890 - val_mean_absolute_error: 1.9890\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5092 - mean_absolute_error: 1.5092 - val_loss: 2.0603 - val_mean_absolute_error: 2.0603\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4563 - mean_absolute_error: 1.4563 - val_loss: 2.0923 - val_mean_absolute_error: 2.0923\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4403 - mean_absolute_error: 1.4403 - val_loss: 2.0045 - val_mean_absolute_error: 2.0045\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4294 - mean_absolute_error: 1.4294 - val_loss: 2.0090 - val_mean_absolute_error: 2.0090\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4276 - mean_absolute_error: 1.4276 - val_loss: 1.9818 - val_mean_absolute_error: 1.9818\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4219 - mean_absolute_error: 1.4219 - val_loss: 2.0014 - val_mean_absolute_error: 2.0014\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4382 - mean_absolute_error: 1.4382 - val_loss: 1.9803 - val_mean_absolute_error: 1.9803\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4304 - mean_absolute_error: 1.4304 - val_loss: 2.0549 - val_mean_absolute_error: 2.0549\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4195 - mean_absolute_error: 1.4195 - val_loss: 2.0461 - val_mean_absolute_error: 2.0461\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4727 - mean_absolute_error: 1.4727 - val_loss: 2.0866 - val_mean_absolute_error: 2.0866\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4734 - mean_absolute_error: 1.4734 - val_loss: 2.0441 - val_mean_absolute_error: 2.0441\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4219 - mean_absolute_error: 1.4219 - val_loss: 2.0611 - val_mean_absolute_error: 2.0611\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3973 - mean_absolute_error: 1.3973 - val_loss: 1.9864 - val_mean_absolute_error: 1.9864\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3909 - mean_absolute_error: 1.3909 - val_loss: 1.9814 - val_mean_absolute_error: 1.9814\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3753 - mean_absolute_error: 1.3753 - val_loss: 2.0253 - val_mean_absolute_error: 2.0253\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3784 - mean_absolute_error: 1.3784 - val_loss: 2.0010 - val_mean_absolute_error: 2.0010\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4132 - mean_absolute_error: 1.4132 - val_loss: 2.0172 - val_mean_absolute_error: 2.0172\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5093 - mean_absolute_error: 1.5093 - val_loss: 2.0731 - val_mean_absolute_error: 2.0731\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3830 - mean_absolute_error: 1.3830 - val_loss: 1.9996 - val_mean_absolute_error: 1.9996\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3788 - mean_absolute_error: 1.3788 - val_loss: 2.0507 - val_mean_absolute_error: 2.0507\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3703 - mean_absolute_error: 1.3703 - val_loss: 2.0029 - val_mean_absolute_error: 2.0029\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3667 - mean_absolute_error: 1.3667 - val_loss: 2.0081 - val_mean_absolute_error: 2.0081\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3614 - mean_absolute_error: 1.3614 - val_loss: 1.9935 - val_mean_absolute_error: 1.9935\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3371 - mean_absolute_error: 1.3371 - val_loss: 1.9912 - val_mean_absolute_error: 1.9912\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3567 - mean_absolute_error: 1.3567 - val_loss: 1.9885 - val_mean_absolute_error: 1.9885\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3467 - mean_absolute_error: 1.3467 - val_loss: 2.0028 - val_mean_absolute_error: 2.0028\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3069 - mean_absolute_error: 1.3069 - val_loss: 1.9882 - val_mean_absolute_error: 1.9882\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3047 - mean_absolute_error: 1.3047 - val_loss: 1.9498 - val_mean_absolute_error: 1.9498\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3145 - mean_absolute_error: 1.3145 - val_loss: 1.9767 - val_mean_absolute_error: 1.9767\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2935 - mean_absolute_error: 1.2935 - val_loss: 2.0454 - val_mean_absolute_error: 2.0454\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2868 - mean_absolute_error: 1.2868 - val_loss: 1.9780 - val_mean_absolute_error: 1.9780\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3037 - mean_absolute_error: 1.3037 - val_loss: 1.9913 - val_mean_absolute_error: 1.9913\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3586 - mean_absolute_error: 1.3586 - val_loss: 2.0363 - val_mean_absolute_error: 2.0363\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3382 - mean_absolute_error: 1.3382 - val_loss: 1.9727 - val_mean_absolute_error: 1.9727\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2893 - mean_absolute_error: 1.2893 - val_loss: 1.9710 - val_mean_absolute_error: 1.9710\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2589 - mean_absolute_error: 1.2589 - val_loss: 1.9966 - val_mean_absolute_error: 1.9966\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3266 - mean_absolute_error: 1.3266 - val_loss: 1.9935 - val_mean_absolute_error: 1.9935\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3006 - mean_absolute_error: 1.3006 - val_loss: 1.9670 - val_mean_absolute_error: 1.9670\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2880 - mean_absolute_error: 1.2880 - val_loss: 1.9634 - val_mean_absolute_error: 1.9634\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2845 - mean_absolute_error: 1.2845 - val_loss: 1.9341 - val_mean_absolute_error: 1.9341\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3055 - mean_absolute_error: 1.3055 - val_loss: 2.0154 - val_mean_absolute_error: 2.0154\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3193 - mean_absolute_error: 1.3193 - val_loss: 1.9657 - val_mean_absolute_error: 1.9657\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2668 - mean_absolute_error: 1.2668 - val_loss: 2.0055 - val_mean_absolute_error: 2.0055\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2737 - mean_absolute_error: 1.2737 - val_loss: 1.9620 - val_mean_absolute_error: 1.9620\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2298 - mean_absolute_error: 1.2298 - val_loss: 1.9631 - val_mean_absolute_error: 1.9631\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3004 - mean_absolute_error: 1.3004 - val_loss: 1.9690 - val_mean_absolute_error: 1.9690\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2671 - mean_absolute_error: 1.2671 - val_loss: 1.9273 - val_mean_absolute_error: 1.9273\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3224 - mean_absolute_error: 1.3224 - val_loss: 2.0007 - val_mean_absolute_error: 2.0007\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2735 - mean_absolute_error: 1.2735 - val_loss: 1.9567 - val_mean_absolute_error: 1.9567\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2041 - mean_absolute_error: 1.2041 - val_loss: 1.9289 - val_mean_absolute_error: 1.9289\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2292 - mean_absolute_error: 1.2292 - val_loss: 1.8946 - val_mean_absolute_error: 1.8946\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2317 - mean_absolute_error: 1.2317 - val_loss: 1.9237 - val_mean_absolute_error: 1.9237\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1985 - mean_absolute_error: 1.1985 - val_loss: 1.9106 - val_mean_absolute_error: 1.9106\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2534 - mean_absolute_error: 1.2534 - val_loss: 2.0031 - val_mean_absolute_error: 2.0031\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2779 - mean_absolute_error: 1.2779 - val_loss: 1.9536 - val_mean_absolute_error: 1.9536\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2113 - mean_absolute_error: 1.2113 - val_loss: 1.9415 - val_mean_absolute_error: 1.9415\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2000 - mean_absolute_error: 1.2000 - val_loss: 1.9491 - val_mean_absolute_error: 1.9491\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1658 - mean_absolute_error: 1.1658 - val_loss: 1.9075 - val_mean_absolute_error: 1.9075\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2166 - mean_absolute_error: 1.2166 - val_loss: 1.9111 - val_mean_absolute_error: 1.9111\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2096 - mean_absolute_error: 1.2096 - val_loss: 1.9531 - val_mean_absolute_error: 1.9531\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2113 - mean_absolute_error: 1.2113 - val_loss: 1.9276 - val_mean_absolute_error: 1.9276\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1719 - mean_absolute_error: 1.1719 - val_loss: 1.8839 - val_mean_absolute_error: 1.8839\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1809 - mean_absolute_error: 1.1809 - val_loss: 1.9834 - val_mean_absolute_error: 1.9834\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2320 - mean_absolute_error: 1.2320 - val_loss: 1.9577 - val_mean_absolute_error: 1.9577\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2530 - mean_absolute_error: 1.2530 - val_loss: 1.9707 - val_mean_absolute_error: 1.9707\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2262 - mean_absolute_error: 1.2262 - val_loss: 1.9157 - val_mean_absolute_error: 1.9157\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3170 - mean_absolute_error: 1.3170 - val_loss: 1.9630 - val_mean_absolute_error: 1.9630\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2478 - mean_absolute_error: 1.2478 - val_loss: 1.9224 - val_mean_absolute_error: 1.9224\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1957 - mean_absolute_error: 1.1957 - val_loss: 1.9541 - val_mean_absolute_error: 1.9541\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1737 - mean_absolute_error: 1.1737 - val_loss: 1.8534 - val_mean_absolute_error: 1.8534\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.1536 - mean_absolute_error: 1.1536 - val_loss: 1.9423 - val_mean_absolute_error: 1.9423\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1789 - mean_absolute_error: 1.1789 - val_loss: 1.8915 - val_mean_absolute_error: 1.8915\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1559 - mean_absolute_error: 1.1559 - val_loss: 1.8898 - val_mean_absolute_error: 1.8898\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1554 - mean_absolute_error: 1.1554 - val_loss: 1.9344 - val_mean_absolute_error: 1.9344\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1516 - mean_absolute_error: 1.1516 - val_loss: 1.9072 - val_mean_absolute_error: 1.9072\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1915 - mean_absolute_error: 1.1915 - val_loss: 1.9073 - val_mean_absolute_error: 1.9073\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1509 - mean_absolute_error: 1.1509 - val_loss: 1.8748 - val_mean_absolute_error: 1.8748\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1276 - mean_absolute_error: 1.1276 - val_loss: 1.9004 - val_mean_absolute_error: 1.9004\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1289 - mean_absolute_error: 1.1289 - val_loss: 1.9225 - val_mean_absolute_error: 1.9225\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1199 - mean_absolute_error: 1.1199 - val_loss: 1.8759 - val_mean_absolute_error: 1.8759\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1650 - mean_absolute_error: 1.1650 - val_loss: 1.8585 - val_mean_absolute_error: 1.8585\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1383 - mean_absolute_error: 1.1383 - val_loss: 1.8529 - val_mean_absolute_error: 1.8529\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1436 - mean_absolute_error: 1.1436 - val_loss: 1.8824 - val_mean_absolute_error: 1.8824\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.1115 - mean_absolute_error: 1.1115 - val_loss: 1.8323 - val_mean_absolute_error: 1.8323\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1252 - mean_absolute_error: 1.1252 - val_loss: 1.8407 - val_mean_absolute_error: 1.8407\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1339 - mean_absolute_error: 1.1339 - val_loss: 1.8628 - val_mean_absolute_error: 1.8628\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1250 - mean_absolute_error: 1.1250 - val_loss: 1.8898 - val_mean_absolute_error: 1.8898\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1815 - mean_absolute_error: 1.1815 - val_loss: 1.8895 - val_mean_absolute_error: 1.8895\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1537 - mean_absolute_error: 1.1537 - val_loss: 1.8572 - val_mean_absolute_error: 1.8572\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1709 - mean_absolute_error: 1.1709 - val_loss: 1.8485 - val_mean_absolute_error: 1.8485\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1124 - mean_absolute_error: 1.1124 - val_loss: 1.8349 - val_mean_absolute_error: 1.8349\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1126 - mean_absolute_error: 1.1126 - val_loss: 1.8680 - val_mean_absolute_error: 1.8680\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1713 - mean_absolute_error: 1.1713 - val_loss: 1.8722 - val_mean_absolute_error: 1.8722\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1053 - mean_absolute_error: 1.1053 - val_loss: 1.9128 - val_mean_absolute_error: 1.9128\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1079 - mean_absolute_error: 1.1079 - val_loss: 1.8463 - val_mean_absolute_error: 1.8463\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0936 - mean_absolute_error: 1.0936 - val_loss: 1.8329 - val_mean_absolute_error: 1.8329\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0558 - mean_absolute_error: 1.0558 - val_loss: 1.8620 - val_mean_absolute_error: 1.8620\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1373 - mean_absolute_error: 1.1373 - val_loss: 1.8090 - val_mean_absolute_error: 1.8090\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1030 - mean_absolute_error: 1.1030 - val_loss: 1.8086 - val_mean_absolute_error: 1.8086\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0752 - mean_absolute_error: 1.0752 - val_loss: 1.8442 - val_mean_absolute_error: 1.8442\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1286 - mean_absolute_error: 1.1286 - val_loss: 1.8714 - val_mean_absolute_error: 1.8714\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1363 - mean_absolute_error: 1.1363 - val_loss: 1.8389 - val_mean_absolute_error: 1.8389\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1391 - mean_absolute_error: 1.1391 - val_loss: 1.8093 - val_mean_absolute_error: 1.8093\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1031 - mean_absolute_error: 1.1031 - val_loss: 1.8519 - val_mean_absolute_error: 1.8519\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1025 - mean_absolute_error: 1.1025 - val_loss: 1.8226 - val_mean_absolute_error: 1.8226\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1152 - mean_absolute_error: 1.1152 - val_loss: 1.8865 - val_mean_absolute_error: 1.8865\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1545 - mean_absolute_error: 1.1545 - val_loss: 1.7915 - val_mean_absolute_error: 1.7915\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1523 - mean_absolute_error: 1.1523 - val_loss: 1.8025 - val_mean_absolute_error: 1.8025\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1989 - mean_absolute_error: 1.1989 - val_loss: 1.8506 - val_mean_absolute_error: 1.8506\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1175 - mean_absolute_error: 1.1175 - val_loss: 1.7709 - val_mean_absolute_error: 1.7709\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0703 - mean_absolute_error: 1.0703 - val_loss: 1.7967 - val_mean_absolute_error: 1.7967\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0569 - mean_absolute_error: 1.0569 - val_loss: 1.7609 - val_mean_absolute_error: 1.7609\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0380 - mean_absolute_error: 1.0380 - val_loss: 1.7985 - val_mean_absolute_error: 1.7985\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0278 - mean_absolute_error: 1.0278 - val_loss: 1.7313 - val_mean_absolute_error: 1.7313\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0388 - mean_absolute_error: 1.0388 - val_loss: 1.7643 - val_mean_absolute_error: 1.7643\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0204 - mean_absolute_error: 1.0204 - val_loss: 1.7497 - val_mean_absolute_error: 1.7497\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0326 - mean_absolute_error: 1.0326 - val_loss: 1.7432 - val_mean_absolute_error: 1.7432\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1121 - mean_absolute_error: 1.1121 - val_loss: 1.8113 - val_mean_absolute_error: 1.8113\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0752 - mean_absolute_error: 1.0752 - val_loss: 1.7434 - val_mean_absolute_error: 1.7434\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0295 - mean_absolute_error: 1.0295 - val_loss: 1.8124 - val_mean_absolute_error: 1.8124\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1292 - mean_absolute_error: 1.1292 - val_loss: 1.8393 - val_mean_absolute_error: 1.8393\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1158 - mean_absolute_error: 1.1158 - val_loss: 1.7968 - val_mean_absolute_error: 1.7968\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0897 - mean_absolute_error: 1.0897 - val_loss: 1.7391 - val_mean_absolute_error: 1.7391\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0154 - mean_absolute_error: 1.0154 - val_loss: 1.7733 - val_mean_absolute_error: 1.7733\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0364 - mean_absolute_error: 1.0364 - val_loss: 1.7573 - val_mean_absolute_error: 1.7573\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0286 - mean_absolute_error: 1.0286 - val_loss: 1.7349 - val_mean_absolute_error: 1.7349\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9993 - mean_absolute_error: 0.9993 - val_loss: 1.7352 - val_mean_absolute_error: 1.7352\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0108 - mean_absolute_error: 1.0108 - val_loss: 1.7739 - val_mean_absolute_error: 1.7739\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0258 - mean_absolute_error: 1.0258 - val_loss: 1.7784 - val_mean_absolute_error: 1.7784\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0180 - mean_absolute_error: 1.0180 - val_loss: 1.7484 - val_mean_absolute_error: 1.7484\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0101 - mean_absolute_error: 1.0101 - val_loss: 1.7471 - val_mean_absolute_error: 1.7471\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1248 - mean_absolute_error: 1.1248 - val_loss: 1.7024 - val_mean_absolute_error: 1.7024\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0163 - mean_absolute_error: 1.0163 - val_loss: 1.7405 - val_mean_absolute_error: 1.7405\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0320 - mean_absolute_error: 1.0320 - val_loss: 1.7122 - val_mean_absolute_error: 1.7122\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9972 - mean_absolute_error: 0.9972 - val_loss: 1.7667 - val_mean_absolute_error: 1.7667\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9839 - mean_absolute_error: 0.9839 - val_loss: 1.7459 - val_mean_absolute_error: 1.7459\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0372 - mean_absolute_error: 1.0372 - val_loss: 1.7179 - val_mean_absolute_error: 1.7179\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9803 - mean_absolute_error: 0.9803 - val_loss: 1.6802 - val_mean_absolute_error: 1.6802\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0082 - mean_absolute_error: 1.0082 - val_loss: 1.7393 - val_mean_absolute_error: 1.7393\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0274 - mean_absolute_error: 1.0274 - val_loss: 1.7190 - val_mean_absolute_error: 1.7190\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0438 - mean_absolute_error: 1.0438 - val_loss: 1.7394 - val_mean_absolute_error: 1.7394\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0155 - mean_absolute_error: 1.0155 - val_loss: 1.7429 - val_mean_absolute_error: 1.7429\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0192 - mean_absolute_error: 1.0192 - val_loss: 1.7442 - val_mean_absolute_error: 1.7442\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0013 - mean_absolute_error: 1.0013 - val_loss: 1.7317 - val_mean_absolute_error: 1.7317\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0123 - mean_absolute_error: 1.0123 - val_loss: 1.7661 - val_mean_absolute_error: 1.7661\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0376 - mean_absolute_error: 1.0376 - val_loss: 1.7194 - val_mean_absolute_error: 1.7194\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0318 - mean_absolute_error: 1.0318 - val_loss: 1.7178 - val_mean_absolute_error: 1.7178\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9714 - mean_absolute_error: 0.9714 - val_loss: 1.6822 - val_mean_absolute_error: 1.6822\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9322 - mean_absolute_error: 0.9322 - val_loss: 1.6857 - val_mean_absolute_error: 1.6857\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9435 - mean_absolute_error: 0.9435 - val_loss: 1.6724 - val_mean_absolute_error: 1.6724\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9302 - mean_absolute_error: 0.9302 - val_loss: 1.6619 - val_mean_absolute_error: 1.6619\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9573 - mean_absolute_error: 0.9573 - val_loss: 1.6774 - val_mean_absolute_error: 1.6774\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9693 - mean_absolute_error: 0.9693 - val_loss: 1.6297 - val_mean_absolute_error: 1.6297\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9748 - mean_absolute_error: 0.9748 - val_loss: 1.6718 - val_mean_absolute_error: 1.6718\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9863 - mean_absolute_error: 0.9863 - val_loss: 1.6295 - val_mean_absolute_error: 1.6295\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.9613 - mean_absolute_error: 0.9613 - val_loss: 1.6874 - val_mean_absolute_error: 1.6874\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0130 - mean_absolute_error: 1.0130 - val_loss: 1.6664 - val_mean_absolute_error: 1.6664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8ea99f2cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 5 Colab Notebook to your Github repository under \"Day 5\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}